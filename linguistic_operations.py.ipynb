{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest_aashay/.local/lib/python2.7/site-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "import spacy\n",
    "import nltk\n",
    "from word2number import w2n\n",
    "from find_similar import *\n",
    "\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "wvec = KeyedVectors.load_word2vec_format(\"../btp2/datasets/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "fl = open(\"small_verb.txt\")\n",
    "seedverbs = []\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "seedtype = {}\n",
    "for line in fl:\n",
    "    seedtype[line.split()[0]] = line.split()[1]\n",
    "    line = line.strip().split(\" \")[0]\n",
    "    seedverbs.append(line.split()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class linguistic_operations():\n",
    "    def __init__(self):\n",
    "        self.depRelations = []\n",
    "        self.lastrelation = []\n",
    "        self.lastGraph = []\n",
    "        self.premGraph = []\n",
    "        self.question = \"\"\n",
    "        self.ind = {}\n",
    "        self.visited = []\n",
    "        self.path_to_jar = \"./stanford-parser/stanford-parser.jar\"\n",
    "        self.path_to_models_jar = \"./stanford-parser/stanford-parser-3.4.1-models.jar\"\n",
    "        self.dependency_parser = StanfordDependencyParser(path_to_jar = self.path_to_jar, path_to_models_jar = self.path_to_models_jar)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def dependencyParse(self,sentence):\n",
    "        self.question = sentence\n",
    "        result = self.dependency_parser.raw_parse(sentence)\n",
    "        dep = result.next()\n",
    "        self.depRelations = list(dep.triples())\n",
    "        return self.depRelations\n",
    "    \n",
    "    \n",
    "    def numNoun(self):\n",
    "        allrelations = {}\n",
    "        lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "        for relation in self.depRelations:\n",
    "            if relation[1] == 'num':\n",
    "                seed = ps.stem(relation[0][0])\n",
    "                if seed not in allrelations:\n",
    "                    allrelations[seed] = []\n",
    "                print (\"rela = \",str(relation[2][0]))\n",
    "                allrelations[seed].append(w2n.word_to_num(str(relation[2][0])))\n",
    "        self.allrelations =  allrelations\n",
    "        return allrelations\n",
    "    \n",
    "    \n",
    "    def findroot(self):\n",
    "        for relation in self.lastrelation:\n",
    "            if relation[0][1][0]=='W':\n",
    "                return ps.stem(relation[0][0])\n",
    "            elif relation[2][1][0]=='W':\n",
    "                return ps.stem(relation[2][0])\n",
    "        return \"NAN\"\n",
    "    \n",
    "    \n",
    "    def makeGraphLast(self):\n",
    "        \n",
    "        last = nltk.sent_tokenize(self.question)[-1]\n",
    "        allwords = self.makeseedvocab(last)\n",
    "        result = self.dependency_parser.raw_parse(last)\n",
    "        dep = result.next()\n",
    "        self.lastrelation = list(dep.triples())\n",
    "        print (self.lastrelation)\n",
    "        newRelation = []\n",
    "        for relation in self.lastrelation:\n",
    "            new = (ps.stem(relation[0][0]),relation[1],ps.stem(relation[2][0]))\n",
    "            newRelation.append(new)\n",
    "\n",
    "        \n",
    "        print (newRelation)\n",
    "        cnt =0 \n",
    "        for token in allwords:\n",
    "            self.ind[token] = cnt\n",
    "            cnt+=1\n",
    "    \n",
    "        graph = [[] for i in range(len(allwords)) ]\n",
    "        for relation in newRelation:\n",
    "            graph[self.ind[relation[0]]].append(self.ind[relation[2]])\n",
    "            graph[self.ind[relation[2]]].append(self.ind[relation[0]])\n",
    "        self.visited = [0 for i in range(len(allwords))]\n",
    "        return graph\n",
    "            \n",
    "    \n",
    "    def dfs(self,root,graph):\n",
    "        self.visited[root] = 1\n",
    "        for i in range(len(graph[root])):\n",
    "            if self.visited[graph[root][i]]==0:\n",
    "                self.dfs(graph[root][i],graph)\n",
    "        return \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    def makeseedvocab(self,last):\n",
    "        allwords = set()\n",
    "        sentences = nltk.word_tokenize(last)\n",
    "        for word in sentences:\n",
    "            allwords.add(ps.stem(word))\n",
    "        print (allwords)\n",
    "        return allwords\n",
    "    \n",
    "    \n",
    "    def whoseQuantity(self):\n",
    "        for relation in self.lastrelation:\n",
    "            if (relation[0][1] == \"NNS\" or relation[0][1] == \"NN\") and ps.stem(relation[0][0]) in self.allrelations:\n",
    "                return ps.stem(relation[0][0])\n",
    "            elif (relation[2][1]==\"NNS\" or relation[2][1]==\"NN\") and ps.stem(relation[2][0]) in self.allrelations:\n",
    "                return ps.stem(relation[2][0])\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:12: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.StanforCoreNLPDependencyParser\u001b[0m instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'passed', u'VBD'), u'ccomp', (u'have', u'VBP')), ((u'have', u'VBP'), u'nsubj', (u'I', u'PRP')), ((u'have', u'VBP'), u'dobj', (u'books', u'NNS')), ((u'books', u'NNS'), u'num', (u'3', u'CD')), ((u'books', u'NNS'), u'cc', (u'and', u'CC')), ((u'books', u'NNS'), u'conj', (u'pens', u'NNS')), ((u'pens', u'NNS'), u'num', (u'4', u'CD')), ((u'passed', u'VBD'), u'nsubj', (u'I', u'PRP')), ((u'passed', u'VBD'), u'dobj', (u'book', u'NN')), ((u'book', u'NN'), u'num', (u'1', u'CD')), ((u'book', u'NN'), u'amod', (u'more', u'JJR')), ((u'passed', u'VBD'), u'prep', (u'to', u'TO')), ((u'to', u'TO'), u'pobj', (u'tom', u'NNP')), ((u'tom', u'NNP'), u'dep', (u'do', u'VBP')), ((u'do', u'VBP'), u'dep', (u'many', u'JJ')), ((u'many', u'JJ'), u'advmod', (u'How', u'WRB')), ((u'do', u'VBP'), u'nsubj', (u'books', u'NNS')), ((u'do', u'VBP'), u'ccomp', (u'have', u'VBP')), ((u'have', u'VBP'), u'nsubj', (u'I', u'PRP')), ((u'have', u'VBP'), u'advmod', (u'now', u'RB'))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate(inp_sent):\n",
    "    \n",
    "    linguistic = linguistic_operations()\n",
    "    x = linguistic.dependencyParse(inp_sent)\n",
    "    allrelation = linguistic.numNoun()\n",
    "    graph = linguistic.makeGraphLast()\n",
    "    qentity = linguistic.whoseQuantity()\n",
    "    trans = findtransition()\n",
    "    findanswer(allrelation,trans,qentity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('rela = ', '3')\n",
      "('rela = ', '4')\n",
      "('rela = ', '1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print (allrelation)\n",
    "# print (linguistic.lastrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# print (linguistic.visited)\n",
    "root = linguistic.findroot()\n",
    "# linguistic.dfs(linguistic.ind[root],graph)\n",
    "print (linguistic.visited)\n",
    "# linguistic.dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['do', 'I', 'how', u'book', 'have', u'mani', 'now', '?'])\n",
      "[((u'have', u'VB'), u'dep', (u'books', u'NNS')), ((u'books', u'NNS'), u'amod', (u'many', u'JJ')), ((u'many', u'JJ'), u'advmod', (u'How', u'WRB')), ((u'have', u'VB'), u'aux', (u'do', u'VBP')), ((u'have', u'VB'), u'nsubj', (u'I', u'PRP')), ((u'have', u'VB'), u'dobj', (u'now', u'RB'))]\n",
      "[(u'have', u'dep', u'book'), (u'book', u'amod', u'mani'), (u'mani', u'advmod', u'how'), (u'have', u'aux', u'do'), (u'have', u'nsubj', u'I'), (u'have', u'dobj', u'now')]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# type(w2n.word_to_num(\"3\"))\n",
    "# x = find_similar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findtransition():\n",
    "        sentences = nltk.sent_tokenize(linguistic.question)[0:-1]\n",
    "        premsentence = \"\"\n",
    "        for i in sentences:\n",
    "            premsentence += i\n",
    "        result = linguistic.dependency_parser.raw_parse(premsentence)\n",
    "        dep = result.next()\n",
    "        deppremise = list(dep.triples())\n",
    "        for relation in deppremise:\n",
    "            if relation[0][1][0:2] ==\"VB\":\n",
    "                return seedtype[x.most_similar(ps.stem(relation[0][0]),seedverbs,wvec)]\n",
    "            if relation[2][1][0:2] ==\"VB\":\n",
    "                return seedtype[x.most_similar(ps.stem(relation[2][0]),seedverbs,wvec)]\n",
    "        return \"0\"\n",
    "def findanswer(allrelations,trans,qentity):\n",
    "    quantities = allrelations[qentity]\n",
    "    if len(quantities)==1:\n",
    "        return float(quantities[0])\n",
    "    ans =0.0\n",
    "    if trans == '+' or trans =='0':\n",
    "        for i in quantities:\n",
    "            ans+=float(i)\n",
    "    if trans == '-':\n",
    "        ans = abs(float(quantities[0])-float(quantities[1]))\n",
    "    return ans\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'passes', 0.7006494402885437), (u'passing', 0.5952332019805908), (u'badly_underthrown', 0.5536887645721436), (u'passed', 0.5499497652053833), (u'dumpoff', 0.535132646560669), (u'passess', 0.5271481275558472), (u'nifty_wraparound', 0.5224149227142334), (u'passs', 0.5210694074630737), (u'eluded_tacklers', 0.49968358874320984), (u'mark_Olukunle', 0.4966002404689789)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'acquiring', 0.7087350487709045), (u'aquire', 0.6920976638793945), (u'acquired', 0.6788821220397949), (u'acquires', 0.6195156574249268), (u'acquisition', 0.6143543720245361), (u'reacquire', 0.6086896657943726), (u'purchase', 0.601489782333374), (u'Acquire', 0.5971124172210693), (u'Acquiring', 0.5743353962898254), (u'buy', 0.5730166435241699)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'get'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x.most_similar(\"acquire\",seedverbs,wvec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
